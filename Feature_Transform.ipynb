{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning / Discretization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression   #We will compare the linear model with decision model on this data\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "X , y = mglearn.datasets.make_wave(n_samples=120)\n",
    "\n",
    "new_data = np.linspace(-3, 3 , 1000, endpoint = False) \n",
    "new_data = new_data.reshape(-1,1) # converts to array of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> How does Linear Regression model relationship between X and y </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mod = LinearRegression().fit(X,y)\n",
    "y_predicted = lr_mod.predict(new_data)\n",
    "plt.plot(new_data, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_mod = DecisionTreeRegressor(max_depth=4).fit(X,y)\n",
    "y_predicted = dt_mod.predict(new_data)\n",
    "plt.plot(new_data, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing model performance\n",
    "print(lr_mod.score(X, y))\n",
    "print(dt_mod.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting both graphs in a single plot\n",
    "\n",
    "Dtreg = DecisionTreeRegressor(min_samples_leaf = 3).fit(X,y)\n",
    "predicted = dt_mod.predict(new_data)\n",
    "plt.plot(new_data, predicted, label=\"Decision Tree\")\n",
    "\n",
    "linreg = LinearRegression().fit(X,y)\n",
    "linpredicted = linreg.predict(new_data)\n",
    "plt.plot(new_data, linpredicted, label = \"Linear Regression\" )\n",
    "\n",
    "plt.plot(X[:,0], y, 'o', c='k')\n",
    "plt.ylabel(\"Regression Output\")\n",
    "plt.xlabel(\"Input Features\")\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning of Input predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kb = KBinsDiscretizer(n_bins=10, strategy='uniform')\n",
    "\n",
    "kb.fit(X)\n",
    "\n",
    "print(\"bin edges \\n\", kb.bin_edges_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the bins defined, we can transform each data point X into a bin using the transform function\n",
    "\n",
    "X_binned = kb.transform(X)\n",
    "X_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binned.toarray()[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_binned.toarray()[0,:])\n",
    "print(pd.DataFrame(X).head(1))   #First data point -0.752759 is stored in bin 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent sparse matrix and create dense matrix, let us re-do the binning with onehot coding. A given input belongs to\n",
    "# one bin and not others. Hence, we can onehot code them\n",
    "\n",
    "kb = KBinsDiscretizer( n_bins = 10, strategy ='uniform', encode ='onehot-dense') \n",
    "kb.fit(X) \n",
    "X_binned = kb.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class assignment\n",
    "\n",
    "Q1: Train a Linear Reg. model with binned inputs?\n",
    "\n",
    "Q2: Use the trained model to predict on new data (new data should be binned as well)\n",
    "\n",
    "Q3: Plot the model predictions\n",
    "\n",
    "Q4: Does the model with binned features perform better than raw features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding slope to the linear model in a bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = np.hstack([X, X_binned])\n",
    "new_data_combined = np.hstack([new_data, new_data_binned])\n",
    "X_combined[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_Combined shape :', X_combined.shape)  #from 10 onehotcoded columns, it has become 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion question\n",
    "\n",
    "What would be the impact of adding the original raw variable with the binned variables? How would the predicted plot look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_combined, y)\n",
    "predicted = linreg.predict(new_data_combined)\n",
    "plt.plot(new_data, predicted)\n",
    "plt.plot(X[:,0], y,'o') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model now has learnt a slope but the slope is same in all the bins (blue line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coefficients for each bin \\n\", linreg.coef_ ) #coefficients for each bin.\n",
    "print(\"\")\n",
    "print(\"intercept\\n\", linreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.score(X_combined, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_product = np.hstack([X_binned, X_binned * X])\n",
    "\n",
    "\n",
    "print(X_product.shape)\n",
    "\n",
    "new_data_product = np.hstack([new_data_binned, new_data_binned * new_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_product[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_binned[1])\n",
    "print(X[1])\n",
    "print(X_binned[1] * X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_product, y)\n",
    "predicted = linreg.predict(new_data_product)\n",
    "plt.plot(new_data, predicted)\n",
    "plt.plot(X[:,0], y,'o') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each bin has it's own offset and its own coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each bin has it's own offset and coefficient\n",
    "\n",
    "print(\"coefficients for each bin \\n\", linreg.coef_ ) #coefficients for each bin.\n",
    "print(\"\")\n",
    "print(\"intercept\\n\", linreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.score(X_product, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial feature transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=5, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = poly.transform(X)\n",
    "\n",
    "new_data_poly = poly.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_poly shape: \", X_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see the difference in the X and X_poly entries\n",
    "\n",
    "print(\"Records in X \\n\", X[0])\n",
    "print(\"Records in X_poly \\n\",X_poly[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns are x, x^2, x ^3\n",
    "\n",
    "print(\"X_Poly feature names : \" , poly.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_poly, y)\n",
    "predicted = linreg.predict(new_data_poly)\n",
    "plt.plot(new_data, predicted)\n",
    "\n",
    "\n",
    "plt.plot(X[:,0], y, 'o')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.score(X_poly, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Q: What happens to the fit as we increase the polynomial degree?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
